# import the required libraries
from bs4 import BeautifulSoup
import requests
import re
import pandas as pd
from random import randint
from time import sleep

base_URL = 'https://startups.co.uk'
 
# get the news category
news_category_URL = f'{base_URL}/news/'

data_list = []

def fetch_article_urls(base_url, lower_page_limit, upper_page_limit):

    data_links  = []

    for i in range(lower_page_limit, upper_page_limit):
        if i == 0:
            current_page = base_url
        else:
            # append category URL to page number to get the current page
            current_page = f'{base_url}page/{i}/'
            
        response = requests.get(current_page)
    
        if response.status_code == 200:
            soup = BeautifulSoup(response.content, 'html.parser')
    
            #  get link cards
            articles = soup.find_all("div",class_="category-childlist-box")
    
            # iterate through link cards to get article unique hrefs
            for article in articles:
                link = article.find('a').get('href')

                # merge article's unique URL with the base URL
                data_links.append(link)
        else:
            print(f'Error fetching links for page {i}: {response.status_code}')

    return data_links

def article_scraper(article_url):
 
    # create a new dictionary for each article
    data = {}  
 
    try:
        sleep(randint(1,10))
        response = requests.get(article_url)
 
        soup = BeautifulSoup(response.content, 'html.parser')

        title_element = soup.find('h1', class_='entry-header-title')
        data['title'] = title_element.text.strip() if title_element else ''
        
        # extract published date, handle NoneType
        published_date_element = soup.find('div', class_='info-block article-date-info')
        data['published_date'] = published_date_element.find('span', class_='meta-value').text.strip() if published_date_element else ''
 
        # extract author, handle NoneType
        author_element_button = soup.find('button', class_='js-open-bio-info user-name')
        author_element_text = soup.find('span', class_='user-name')

        if author_element_button:
            data['author'] = author_element_button.text.strip()
        elif author_element_text:
            data['author'] = author_element_text.text.strip()
        else:
            data['author'] = ''
 
        # extract content, handle NoneType
        content_element = soup.find('div', class_='entry-content')
        data['content'] = content_element.text if content_element else ''
 
        # append the data for this article to the list
        data_list.append(data)

    except requests.RequestException as e:
        print(f'Error fetching article data for {article_url}: {e}')


article_urls = fetch_article_urls(news_category_URL, 80, 100)

#article_scraper(article_urls[0])
for article in article_urls:
    article_scraper(article)

#print(data_list)
df = pd.DataFrame(data_list)
csv_file = 'output_startupscouk_new5.csv'
df.to_csv(csv_file, index=False)  